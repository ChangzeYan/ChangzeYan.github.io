---
title: 实体识别
author: ChangzeYan
date: 2021-02-10 09:59:13
tags: NLP
categories: Python
cover:
---

# 实体识别工具

## [Pyhanlp](https://github.com/hankcs/pyhanlp)

### 安装
先安装JDK。你需要保证JDK位数、操作系统位数和Python位数一致。然后设置JAVA_HOME环境变量，最后执行：
```bash
# (可选)conda安装jpype1更方便
conda install -c conda-forge jpype1==0.7.0      
pip install pyhanlp
```
    
检测是否安装成功：
```bash
hanlp
```
如果安装成功，会自动下载词典

![安装成功](https://github.com/ChangzeYan/ChangzeYan.github.io/raw/hexo/source/pic/hanlp-install-success.png)

如果报RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb，需要升级 numpy：

```bash
pip install --upgrade numpy
```

### 交互模式
```bash
hanlp segment
```
![交互模式](https://github.com/ChangzeYan/ChangzeYan.github.io/raw/hexo/source/pic/hanlp-segment.png)

### hanlp词性表

参考：[hanlp词性表](https://blog.csdn.net/u014258362/article/details/81044286)


## StanfordCoreNLP
### 安装

```bash
 pip install stanfordcorenlp 
```
由于其源码为 JAVA 编写，所以需要 JDK1.8 及以上版本的支持，下载安装 JDK1.8。
下载 [StanfordCoreNLP 的相关文件](https://stanfordnlp.github.io/CoreNLP/)，解压到任意目录

如果要处理中文，还需下载中文jar包：[stanford-chinese-corenlp-2018-10-05-models](http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-10-05-models.jar)，下载完放到上一步解压后的文件夹中。

### 调用
```python
from stanfordcorenlp import StanfordCoreNLP
 
# 加载模型，路径是上面解压的路径
stanford_model = StanfordCoreNLP(r'./stanford-corenlp-full-2018-02-27', lang='zh')
 
text = "张三和李四在2019年3月23日在北京的腾讯技术有限公司一起开会。"
 
res = stanford_model.ner(text)
```
参数：
处理英文时：lang='en'

## kashgari
参考：
- [Kashgari:一个方便快捷的命名实体识别、文本分类工具包](https://blog.csdn.net/qq_27492735/article/details/108767237)
- [NER-使用kashgari中的Bert+BiLSTM+CRF](https://www.cnblogs.com/mingriyingying/p/13379434.html)
- [五分钟搭建一个基于BERT的NER模型](https://www.jianshu.com/p/1d6689851622)

### 环境准备
```bash
conda create --envs myTestNER python==3.6
pip insall tensorflow==1.14.0
pip install kashgari==1.1.5
```

### 下载模型
[中文模型](https://github.com/ymcui/Chinese-BERT-wwm/)

![中文模型下载](https://github.com/ChangzeYan/ChangzeYan.github.io/raw/hexo/source/pic/Chinese-BERT-wwm.png)

[英文模型](https://github.com/google-research/bert)

### 数据格式
训练集、测试集、验证集的格式均为：
```
我 O
们 O
变 O
而 O
以 O
书 O
会 O
友 O
， O
把 O
欧 B-LOC
美 B-LOC
流 O
行 O
的 O
食 O
品 O
类 O
图 O
谱 O
、 O
画 O
册 O
```

### 读取数据
```python
from typing import Tuple, List

# 定义一个读取数据的类
class DataReader(object):
    def read_file(file_path: str,
                  text_index: int = 0,
                  label_index: int = 1) -> Tuple[List[List[str]], List[List[str]]]:
        """
        根据文件路径读取训练数据、测试数据以及验证数据的text和label
        """
        x_data, y_data = [], []
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()  # 以splitlines的方式获取数据
            x, y = [], []
            for line in lines:
                rows = line.split(' ')  # 以空格进行切分
                if len(rows) == 1:
                    x_data.append(x)
                    y_data.append(y)
                    x = []
                    y = []
                else:
                    x.append(rows[text_index])
                    y.append(rows[label_index])
        return x_data, y_data
```

### 训练
```python
import kashgari
from kashgari import utils
from kashgari.embeddings import BERTEmbedding, BERTEmbeddingV2
from kashgari.tasks.labeling import BiLSTM_CRF_Model
from data_load import DataReader

kashgari.config.use_cudnn_cell = False

# 加载训练数据、验证数据以及测试数据

train_x, train_y = DataReader.read_file('../data/train.txt')
test_x, test_y = DataReader.read_file('../data/test.txt')
valid_x, valid_y = DataReader.read_file('../data/new_val.txt')

print("train data count: {len(train_x)}")
print("validate data count: {len(valid_x)}")
print("test data count: {len(test_x)}")

# 利用kashgari创建Bert Embedding

bert_embed = BERTEmbedding('chinese_wwm_ext_L-12_H-768_A-12',
                           task=kashgari.LABELING,
                           sequence_length=100)

# 调用BiLSTM_CRF_Model模型并进行训练

model = BiLSTM_CRF_Model(bert_embed)

model.fit(x_train=train_x, y_train=train_y,
          x_validate=valid_x, y_validate=valid_y,
          batch_size=128, epochs=2)
model.save('./model/NER.h5')

model.evaluate(test_x, test_y)

model.predict([['小', '明', '在', '商', '务', '部']])

```

### 使用自带的人民日报数据
```python
import jieba
from kashgari.tasks.seq_labeling import BLSTMCRFModel
from kashgari.corpus import ChinaPeoplesDailyNerCorpus
from kashgari.embeddings import BERTEmbedding
embedding = BERTEmbedding('/home/eee/sentence-alignment-classification-model/model/multi_cased_L-12_H-768_A-12', 100)
 
train_x, train_y = ChinaPeoplesDailyNerCorpus.get_sequence_tagging_data('train')
validate_x, validate_y = ChinaPeoplesDailyNerCorpus.get_sequence_tagging_data('validate')
test_x, test_y  = ChinaPeoplesDailyNerCorpus.get_sequence_tagging_data('test')
 
model = BLSTMCRFModel(embedding)
model.fit(train_x,
          train_y,
          validate_y=validate_y,
          validate_x=validate_x,
          epochs=200,
          batch_size=500)
model.save('./model')
 
new_model = BLSTMCRFModel.load_model('./model')
 
# EXAMPLE 1
news = "「DeepMind 击败人类职业玩家的方式与他们声称的 AI 使命，以及所声称的『正确』方式完全相反。」"
x = list(jieba.cut(news))
>>> x
['「', 'DeepMind', ' ', '击败', '人类', '职业', '玩家', '的', '方式', '与', '他们', '声称', '的', ' ', 'AI', ' ', '使命', '，', '以及', '所', '声称', '的', '『', '正确', '』', '方式', '完全', '相反', '。', '」']
>>> new_model.predict(x)                                                                                                     
['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
 
# EXAMPLE 2
news = "陈志衍是有个非常好的男孩子，他住在香港的九龙塘区，他今年二十三号生日。"
x = list(jieba.cut(news))
>>> x
['陈志衍', '是', '有', '个', '非常', '好', '的', '男孩子', '，', '他', '住', '在', '香港', '的', '吉林', '区', '，', '他', '今年', '二十三', '号', '生日', '。']
>>> new_model.predict(x)
['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
```

## [stanza](https://mp.weixin.qq.com/s/HcX8b84YazwVI-usWsCUew)


## LTP
参考：[LTP4 文档](https://ltp.readthedocs.io/zh_CN/latest/index.html)

### 安装
```
pip install ltp
```

### 加载模型
```
from ltp import LTP
ltp = LTP() # 默认加载 Small 模型
# ltp = LTP(path = "base|small|tiny")
# ltp = LTP(path = "tiny.tgz|tiny-tgz-extracted") # 其中 tiny-tgz-extracted 是 tiny.tgz 解压出来的文件夹
```

### 命名实体识别
```
from ltp import LTP

ltp = LTP()

seg, hidden = ltp.seg(["他叫汤姆去拿外衣。"])
ner = ltp.ner(hidden)
# [['他', '叫', '汤姆', '去', '拿', '外衣', '。']]
# [[('Nh', 2, 2)]]

tag, start, end = ner[0][0]
print(tag,":", "".join(seg[0][start:end + 1]))
# Nh : 汤姆
```